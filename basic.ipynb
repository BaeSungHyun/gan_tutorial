{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the intermediate value y\n",
    "dz_dy = t.gradient(z, y)\n",
    "# make sure that the resulting derivative, 2*y, = sum(x)*2 = 8\n",
    "assert dz_dy.numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x*x\n",
    "    z = y*y\n",
    "dz_dx = t.gradient(z, x)\n",
    "dy_dx = t.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(108.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dz_dx)\n",
    "print(dy_dx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Backpropagation : update interior weights within the network in a principled way, it has several shortcomings\n",
    " that made deep networks difficult to use in practice. \n",
    " 1. Vanishing Gradient -> one of the solution: ReLU\n",
    "\n",
    " 2. How network utilizes its available free paremters\n",
    " P(Y|X) = P(X|Y)*P(Y) / P(X) => posterior probability = likelihood * prior probability (distribution)\n",
    " To say this in other words, the output of the neuron = all the input values * distributions on inputs\n",
    "\n",
    "A problem occurs when those values become tightly coupled. This makes it intractable to compute the relative\n",
    "contribution of different parameters, particularly in a deep network. One of the solution: Boltzmann machine\n",
    "\n",
    "Other solutions: Support Vector Machines, Gradient and Stochastic Gradient Boosting Models, \n",
    "                Random Forests, Penalized Regression Methods such as LASSO and Elastic Net\n",
    "\n",
    "In theory, deep neural networks had potentially greater power since they have a lot of \"deep\" layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing AlexNet\n",
    "\n",
    "It is true that ReLU can solve vanishing gradient problem, but ReLU functions have the downside that they \n",
    "can \"turn off\" if the input falls below 0.\n",
    "y = x if x > 0, else 0.01x        or PReLU(Parameterized Leak ReLU): y = max(ax,x) if a <= 1\n",
    "\n",
    "Another trick used by AlexNet is dropout. Idea of dropout is inspired by ensemble methods in which we average \n",
    "the predictions of many models to obtain more robust results. Cleary for deep neural networks, this is prohibitive. Compromise is to randomly set the values of a subset of neurons to 0 with a probability of 0.5\n",
    "\n",
    "Another enhancement used in AlexNet is local response normalization. Even though ReLUs don't saturate in the same manner as other units, the authors of the model still found value in constraining the range of output. \n",
    "For example, in an individual kernel, they normalized the input using values of adjacent kernels, meaning the \n",
    "overall response was rescaled. This is similar to batch normalization, which applies a transformation on \"raw\" activations within a network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
