{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the intermediate value y\n",
    "dz_dy = t.gradient(z, y)\n",
    "# make sure that the resulting derivative, 2*y, = sum(x)*2 = 8\n",
    "assert dz_dy.numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x*x\n",
    "    z = y*y\n",
    "dz_dx = t.gradient(z, x)\n",
    "dy_dx = t.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(108.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dz_dx)\n",
    "print(dy_dx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Backpropagation : update interior weights within the network in a principled way, it has several shortcomings\n",
    " that made deep networks difficult to use in practice. \n",
    " 1. Vanishing Gradient -> one of the solution: ReLU\n",
    "\n",
    " 2. How network utilizes its available free paremters\n",
    " P(Y|X) = P(X|Y)*P(Y) / P(X) => posterior probability = likelihood * prior probability (distribution)\n",
    " To say this in other words, the output of the neuron = all the input values * distributions on inputs\n",
    "\n",
    "A problem occurs when those values become tightly coupled. This makes it intractable to compute the relative\n",
    "contribution of different parameters, particularly in a deep network. One of the solution: Boltzmann machine\n",
    "\n",
    "Other solutions: Support Vector Machines, Gradient and Stochastic Gradient Boosting Models, \n",
    "                Random Forests, Penalized Regression Methods such as LASSO and Elastic Net\n",
    "\n",
    "In theory, deep neural networks had potentially greater power since they have a lot of \"deep\" layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing AlexNet\n",
    "\n",
    "It is true that ReLU can solve vanishing gradient problem, but ReLU functions have the downside that they \n",
    "can \"turn off\" if the input falls below 0.\n",
    "y = x if x > 0, else 0.01x        or PReLU(Parameterized Leak ReLU): y = max(ax,x) if a <= 1\n",
    "\n",
    "Another trick used by AlexNet is dropout. Idea of dropout is inspired by ensemble methods in which we average \n",
    "the predictions of many models to obtain more robust results. Cleary for deep neural networks, this is prohibitive. Compromise is to randomly set the values of a subset of neurons to 0 with a probability of 0.5\n",
    "\n",
    "Another enhancement used in AlexNet is local response normalization. Even though ReLUs don't saturate in the same manner as other units, the authors of the model still found value in constraining the range of output. \n",
    "For example, in an individual kernel, they normalized the input using values of adjacent kernels, meaning the \n",
    "overall response was rescaled. This is similar to batch normalization, which applies a transformation on \"raw\" activations within a network. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just say for Natural Language Processing you need to take account of past context. It is the reason why RNN was developed. But RNN only takes account of prior hidden layer. So to improve that, LSTM was developed. Unlike feedforward networks, RNNs aren't trained with traditional backpropagation, but rather a variant known as 'backpropagation through time'(BPTT)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Procedure\n",
    "\n",
    "1. How to initialize the weights\n",
    "        ex random weights with some range - at least to find local minimum\n",
    "2. How to find the local minimum loss\n",
    "        ex gradient descent\n",
    "\n",
    "To improve SGD, Nesterov Momentum was introduced. It is the idea to use a form of exponentially weighted momentum that remembers prior steps and continues in promising directions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Gradient (Adagrad)\n",
    "scales the learning rate for each update by the running the sum of squares of the gradient of that parameter;\n",
    "thus elements that are frequently updated are downsampled, while those that are infrequently updated are pushed to update with greater magnitude. For ReLU, there is problem of exploding gradient. So if you use Adagrad with ReLU, learning rate is easily vanished. To prevent this, two variant methods, RMSProp(RNN) AdaDelta imposed fixed-width windows of n steps in the computation of G."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Boss: Adaptive Momentum Estimation (ADAM): combine momentum + AdaDelta: the momentum calculation is used to preserve the history of past gradient updates, while the sum of decaying squared gradients within a fixed update window used in AdaDelta is applied to scale the resulting gradient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about weight initialization. In earlier research, it was common to initialize weights in a neural network with some range of random values. Breakthrough was to use pre-traiing before backpropagation.\n",
    "\n",
    "Tensorflow Keras module initializes weights from either a truncated normal distribution or uniform distribution. This comes from activation functions such as sigmoidal and hyperbolic can be easily saturated. Then our mission is to keep the weights in such a range that they don't saturate the neuron's output. In other words, assume that input and output values of the neuron have similar variance. The signal shouldn't be massively amplifying or diminishing while passing neurons.\n",
    "I am skipping little bit, we can use a truncated normal or uniform distribution with variance 1/N(number of weights or the average number of input and output units)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
